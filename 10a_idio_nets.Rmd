---
title: "10a_idio_nets"
author: "Sglatt"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Packages
```{r packages}
if (!require("dplyr")) {
  install.packages("dplyr")
  require("dplyr")
}
if (!require("lubridate")) {
  install.packages("lubridate")
  require("lubridate")
}
if (!require("purrr")) {
  install.packages("purrr")
  require("purrr")
}
if (!require("tidyr")) {
  install.packages("tidyr")
  require("tidyr")
}
if (!require("qgraph")) {
  install.packages("qgraph")
  require("qgraph")
}
if (!require("graphicalVAR")) {
  install.packages("graphicalVAR")
  require("graphicalVAR")
}
if (!require("ggplot2")) {
  install.packages("ggplot2")
  require("ggplot2")
}
if (!require("tibble")) {
  install.packages("tibble")
  require("tibble")
}
```

# Import the dataset
```{r data}
dataset <- read.csv("10_raw_materials/PT_EMA 2022-05-17 full cleaned with controls without _F.csv")
colnames(dataset)

# We will be using the symptoms:
# fowg = I am terrified of gaining weight.
# drivethin = I am preoccupied with the desire to be thinner.
# bodydiss = I do not like how my body looks.
# bodycheck = I frequently check to see if my body has changed (e.g., by pinching).
# heartrace = My heart races for no good reason.
# interocept = I am very sensitive to changes in my internal bodily sensations. (trust/mistrust in body sensations).
# pays_sens = I don't like the physical sensations I feel when eating. (trust/mistrust in body sensations).
# feelfat = I feel fat

colnames(dataset)
View(dataset)

# Filter the dataset to ID (IDclean), day/beep variables (dayvar, beepvar), and those symptoms
filtered_dataset <- dataset %>%
  select(
    IDclean, dayvar, beepvar,
    fowg, drivethin, bodydiss, bodycheck,
    heartrace, interocept, pays_sens, feelfat
  )

# How many people are in this dataset?
length(unique(filtered_dataset$IDclean)) # 128

# Remove controls
filtered_dataset <- filtered_dataset %>%
  filter(!grepl("^PTC", IDclean))
length(unique(filtered_dataset$IDclean)) # 75

View(filtered_dataset)

first_valid_row <- which(!is.na(filtered_dataset$feelfat))[1]
first_valid_row
filtered_dataset$IDclean[first_valid_row]
```

# Look at missingness for each person for those six symptoms
```{r missing}
missing_per_id <- filtered_dataset %>%
  select(-dayvar, -beepvar) %>%
  group_by(IDclean) %>%
  summarise(
    total_vals = n() * 6,
    missing_vals = sum(is.na(c_across(fowg:feelfat))),
    percent_missing = round((missing_vals / total_vals) * 100, 2)
  )

# This will show you the the total number of observations for each person and the  ~approximate % missing. >50% missing may not converge in analyses
missing_per_id
```

# Mean and SD for those six symptoms for each person
```{r m sd}
descriptives_per_id <- filtered_dataset %>%
  select(IDclean, fowg, drivethin, bodydiss, bodycheck, heartrace, interocept, pays_sens, feelfat) %>%
  group_by(IDclean) %>%
  summarise(
    fowg_mean = mean(fowg, na.rm = TRUE),
    fowg_sd = sd(fowg, na.rm = TRUE),
    drivethin_mean = mean(drivethin, na.rm = TRUE),
    drivethin_sd = sd(drivethin, na.rm = TRUE),
    bodydiss_mean = mean(bodydiss, na.rm = TRUE),
    bodydiss_sd = sd(bodydiss, na.rm = TRUE),
    bodycheck_mean = mean(bodycheck, na.rm = TRUE),
    bodycheck_sd = sd(bodycheck, na.rm = TRUE),
    heartrace_mean = mean(heartrace, na.rm = TRUE),
    heartrace_sd = sd(heartrace, na.rm = TRUE),
    interocept_mean = mean(interocept, na.rm = TRUE),
    interocept_sd = sd(interocept, na.rm = TRUE),
    pays_sens_mean = mean(pays_sens, na.rm = TRUE),
    pays_sens_sd = sd(pays_sens, na.rm = TRUE),
    feelfat_mean = mean(feelfat, na.rm = TRUE),
    feelfat_sd = sd(feelfat, na.rm = TRUE),
    .groups = "drop"
  )

# This will show you the mean and standrad deviation of every symptom for every person.
descriptives_per_id

# Now, we should check if anyone has symptoms with a standard deviation close to 0.
# standard deviations close to 0 can lead to convergence issues in the estimation of the network model

# Pulling the SD information from the descriptive's data frame
sd_data <- descriptives_per_id %>%
  select(IDclean, ends_with("_sd"))

# Changing that to long format
sd_long <- sd_data %>%
  pivot_longer(
    cols = ends_with("_sd"),
    names_to = "symptom",
    values_to = "sd"
  )

# Now, find the people-symptom pairs that have low SDs
low_sd <- sd_long %>%
  filter(sd < 1)

# This will show you the symptoms that have low SDs for each person
# For example, in the first row, PT004 has an SD of 2.53 for bodycheck and pays_sens
low_sd
```

# Check for people with >1 day-beep pair
```{r duplicate obs}
duplicates <- filtered_dataset %>%
  group_by(IDclean, dayvar, beepvar) %>%
  filter(n() > 1) %>%
  arrange(IDclean, dayvar, beepvar)

duplicates

# How many IDs have this problem?
length(unique(duplicates$IDclean)) # 15

# Who are they?
unique(duplicates$IDclean)
# "PT001" "PT006" "PT007" "PT012" "PT013" "PT015" "PT018" "PT019" "PT025" "PT029" "PT030" "PT035" "PT047" "PT048" "PT056"

# Exclude those IDs from the dataset
xclude_ids <- unique(duplicates$IDclean)

# Create a clean dataset without those participants
filtered_dataset_nodupes <- filtered_dataset %>%
  filter(!(IDclean %in% xclude_ids))
```

# Pick an ID for analysis
```{r ID}
# Get a list of all the IDs in the dataset
sort(unique(filtered_dataset_nodupes$IDclean))
length(unique(filtered_dataset_nodupes$IDclean)) #

id_name <- "PT062" # put the ID that you want to do analysis for

# put the location where you want the results to be saved
# This line will save the results in a folder named after the ID
output_dir <- file.path("~/Box/Interoceptive Abstract/Network_results", id_name)

if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Select the data for that ID
data_network <- filtered_dataset_nodupes %>%
  filter(IDclean == id_name) %>%
  select(-IDclean) # drop ID column

view(data_network)

# Define the symptoms
variable_names <- c("fowg", "drivethin", "bodydiss", "bodycheck", "heartrace", "interocept", "pays_sens", "feelfat")

# Idiographic networks
network <- graphicalVAR(
  data_network,
  vars = variable_names,
  dayvar = "dayvar",
  beepvar = "beepvar",
  gamma = 0,
  verbose = FALSE
)

# save graphs separately
# Contemporaneous network (PCC)
pdf(file = file.path(output_dir, paste0(id_name, "_network_PCC.pdf")), height = 5, width = 8)
pcc <- plot(network, "PCC",
  labels = variable_names,
  edge.labels = TRUE,
  layout = "spring",
  label.cex = 1.3,
  alpha = 0.05,
  vsize = 9
)
dev.off()

# Temporal network (PDC)
pdf(file = file.path(output_dir, paste0(id_name, "_network_PDC.pdf")), height = 5, width = 8)
pdc <- plot(network, "PDC",
  labels = variable_names,
  label.cex = 1.3,
  alpha = 0.05,
  vsize = 9,
  edge.labels = TRUE,
  repulsion = 1.5
)
dev.off()


# Create and save centrality plots
centralityplot <- plot(network,
  labels = variable_names,
  edge.labels = TRUE,
  layout = "spring"
) %>%
  centralityPlot() +
  theme(legend.position = "none") +
  theme(
    axis.text = element_text(size = 13),
    strip.text.y = element_text(size = 15),
    axis.text.x = element_text(angle = 90),
    strip.text.x = element_text(size = 15)
  )

ggsave(filename = file.path(output_dir, paste0(id_name, "_centralityplot.png")), plot = centralityplot)

# Centrality tables
# PCC centrality
pcc_cent <- centrality_auto(pcc)[[1]]$node.centrality %>%
  rownames_to_column("nodes") %>%
  mutate(type = "PCC") %>%
  relocate(type)

# PDC centrality
pdc_cent <- centrality_auto(pdc)[[1]]$node.centrality %>%
  rownames_to_column("nodes") %>%
  mutate(type = "PDC") %>%
  relocate(type)

# Combine and export
centr <- plyr::rbind.fill(pcc_cent, pdc_cent)
write.csv(centr, file.path(
  output_dir,
  paste0(id_name, "_centralitytables.csv")
),
row.names = FALSE
)

# Edge tables

# PCC edges
pcc_edges <- data.frame(pcc[[1]][[1]]) %>%
  mutate(
    from_var = variable_names[as.integer(from)],
    to_var   = variable_names[as.integer(to)]
  ) %>%
  relocate(from_var, to_var, .after = to)


write.csv(pcc_edges, file.path(
  output_dir,
  paste0(id_name, "_pcc_edges.csv")
),
row.names = TRUE
)

# PDC edges
pdc_edges <- data.frame(pdc[[1]][[1]]) %>%
  mutate(
    from_var = variable_names[as.integer(from)],
    to_var   = variable_names[as.integer(to)]
  ) %>%
  relocate(from_var, to_var, .after = to)

write.csv(pdc_edges, file.path(
  output_dir,
  paste0(id_name, "_pdc_edges.csv")
),
row.names = TRUE
)

# Save workspace
save.image(file = file.path(
  output_dir,
  paste0(id_name, ".RData")
))
```

# If you want to use a loop: 
```{r loop}
# list the IDs you want to use on a loop
id_list <- c(
  "PT003", "PT004", "PT005", "PT008", "PT009", "PT014", "PT016", "PT017", "PT020",
  "PT021", "PT022", "PT023", "PT024", "PT026", "PT028", "PT031", "PT032", "PT033",
  "PT034", "PT036", "PT037", "PT039", "PT040", "PT042", "PT043", "PT044", "PT045",
  "PT046", "PT049", "PT050", "PT051", "PT053", "PT054", "PT055", "PR059"
)

id_list_2 <- c(
  # "PT057", missing too much data - only has 23%. All variables had sufficient SD
  # "PT058", missing too much data - only has 15%. All variables had sufficient SD
  # "PT059",
  # "PT060", model didnt converge, all SDs were fine, only 33% data
  #  "PT062", didnt coverge got stuck
  # "PT064", "PT065", "PT068",
  # "PT069", "PT070", "PT072", "PT073", "PT075", "PT080", "PT081", "PT082",
  # "PT084",
  # "PT085",
  # "PT086", ddint cob verge got stuck
  "PT088", "PT089", "PT090", "PT092", "PT095",
  "PT096", "PT097"
)

filtered_dataset_nodupes %>%
  filter(IDclean == "PT086") %>%
  summarise(across(
    c(
      "fowg", "drivethin", "bodydiss", "bodycheck",
      "heartrace", "interocept", "pays_sens", "feelfat"
    ),
    list(
      sd = ~ sd(., na.rm = TRUE),
      non_missing_pct = ~ mean(!is.na(.)) * 100
    ),
    .names = "{.col}_{.fn}"
  )) %>%
  pivot_longer(
    everything(),
    names_to = c("variable", ".value"),
    names_pattern = "^(.*)_(sd|non_missing_pct)$"
  )

skipped_ids_2 <- c() # list to track skipped IDs

# Loop through each of those iDs
for (id_name in id_list_2) {
  cat("Running analysis for", id_name, "...\n")

  # Set output directory
  # Create a folder for each ID
  output_dir <- file.path("~/Box/Interoceptive Abstract/Network_results", id_name)
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }

  # Filter the dataset
  data_network <- filtered_dataset_nodupes %>%
    filter(IDclean == id_name) %>%
    select(-IDclean)

  # Define variable names
  variable_names <- c("fowg", "drivethin", "bodydiss", "bodycheck", "heartrace", "interocept", "pays_sens", "feelfat") # Do NOT include feelfat up until PT057 (it was only added to the  battery after PT055. PT059 was also still missing it)

  # SKIP if participant has no data
  if (nrow(data_network) == 0) {
    cat("No data for", id_name, "- skipping.\n")
    skipped_ids_2 <- c(skipped_ids_2, paste0(id_name, " (no data)"))
    next
  }

  # Try-catch block --
  # So, if one ID has insufficient data or causes model convergence failure, the loop will continue
  result <- try(
    {
      network <- graphicalVAR(
        data_network,
        vars = variable_names,
        dayvar = "dayvar",
        beepvar = "beepvar",
        gamma = 0,
        verbose = FALSE
      )

      # Contemporaneous lot
      pdf(file = file.path(output_dir, paste0(id_name, "_network_PCC.pdf")), height = 5, width = 8)
      pcc <- plot(network, "PCC",
        labels = variable_names,
        edge.labels = TRUE,
        layout = "spring",
        label.cex = 1.3,
        alpha = 0.05,
        vsize = 9
      )
      dev.off()

      # Temporal plot
      pdf(file = file.path(output_dir, paste0(id_name, "_network_PDC.pdf")), height = 5, width = 8)
      pdc <- plot(network, "PDC",
        labels = variable_names,
        label.cex = 1.3,
        alpha = 0.05,
        vsize = 9,
        edge.labels = TRUE,
        repulsion = 1.5
      )
      dev.off()

      # Centrality plot
      centralityplot <- plot(network,
        labels = variable_names,
        edge.labels = TRUE,
        layout = "spring"
      ) %>%
        centralityPlot() +
        theme(legend.position = "none") +
        theme(
          axis.text = element_text(size = 13),
          strip.text.y = element_text(size = 15),
          axis.text.x = element_text(angle = 90),
          strip.text.x = element_text(size = 15)
        )

      ggsave(filename = file.path(output_dir, paste0(id_name, "_centralityplot.png")), plot = centralityplot)

      # Centrality tables
      pcc_cent <- centrality_auto(pcc)[[1]]$node.centrality %>%
        rownames_to_column("nodes") %>%
        mutate(type = "PCC") %>%
        relocate(type)

      pdc_cent <- centrality_auto(pdc)[[1]]$node.centrality %>%
        rownames_to_column("nodes") %>%
        mutate(type = "PDC") %>%
        relocate(type)

      centr <- plyr::rbind.fill(pcc_cent, pdc_cent)
      write.csv(centr, file.path(output_dir, paste0(id_name, "_centralitytables.csv")), row.names = FALSE)

      # Edge tables

      pcc_edges <- data.frame(pcc[[1]][[1]]) %>%
        mutate(
          from_var = variable_names[as.integer(from)],
          to_var   = variable_names[as.integer(to)]
        ) %>%
        relocate(from_var, to_var, .after = to)

      write.csv(pcc_edges, file.path(output_dir, paste0(id_name, "_pcc_edges.csv")), row.names = TRUE)

      pdc_edges <- data.frame(pdc[[1]][[1]]) %>%
        mutate(
          from_var = variable_names[as.integer(from)],
          to_var   = variable_names[as.integer(to)]
        ) %>%
        relocate(from_var, to_var, .after = to)

      write.csv(pdc_edges, file.path(output_dir, paste0(id_name, "_pdc_edges.csv")), row.names = TRUE)

      save.image(file = file.path(output_dir, paste0(id_name, ".RData")))
    },
    silent = TRUE # This allows the code to continue running with the next ID if someone has no data/non-convergence
  )

  # & this tracks IDs that fail due to error (e.g., convergence problems)
  if (inherits(result, "try-error")) {
    cat("Error for", id_name, "- skipping.\n")
    skipped_ids_2 <- c(skipped_ids_2, paste0(id_name, " (error)"))
  }
}

# Show who was skipped
skipped_ids_2
```

